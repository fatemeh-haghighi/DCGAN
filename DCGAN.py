# -*- coding: utf-8 -*-
"""NNDL_miniproj3_Q2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Fe4lg4JDEcokR_vp_a3mEoLn9ec4AbNu

## loading libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
# %matplotlib inline
import matplotlib.pyplot as plt
from keras.datasets import cifar10
from keras.models import Sequential, Model
from keras.layers import Input, Dense, LeakyReLU, BatchNormalization, ReLU
from keras.layers import Conv2D, Conv2DTranspose, Reshape, Flatten
from keras.optimizers import Adam
from keras import initializers
from keras.utils import plot_model, np_utils
from keras import backend as keras

"""## define generator for model"""

def generator_model():
  init = initializers.RandomNormal(stddev=0.02)
  generator = Sequential()
  generator.add(Dense(2*2*512, input_shape=(100,), kernel_initializer=init))
  generator.add(Reshape((2, 2, 512)))
  generator.add(BatchNormalization())
  generator.add(LeakyReLU(0.2))
  generator.add(Conv2DTranspose(256, kernel_size=5, strides=2, padding='same'))
  generator.add(BatchNormalization())
  generator.add(LeakyReLU(0.2))
  generator.add(Conv2DTranspose(128, kernel_size=5, strides=2, padding='same'))
  generator.add(BatchNormalization())
  generator.add(LeakyReLU(0.2))
  generator.add(Conv2DTranspose(64, kernel_size=5, strides=2, padding='same'))
  generator.add(BatchNormalization())
  generator.add(LeakyReLU(0.2))
  generator.add(Conv2DTranspose(3, kernel_size=5, strides=2, padding='same',activation='tanh'))
  return generator

"""## define discriminator for model"""

def discriminator_model(shape):
  init = initializers.RandomNormal(stddev=0.02)
  discriminator = Sequential()
  discriminator.add(Conv2D(64, kernel_size=5, strides=2, padding='same',input_shape=(shape), kernel_initializer=init))
  discriminator.add(LeakyReLU(0.2))
  discriminator.add(Conv2D(128, kernel_size=5, strides=2, padding='same'))
  discriminator.add(BatchNormalization())
  discriminator.add(LeakyReLU(0.2))
  discriminator.add(Conv2D(256, kernel_size=5, strides=2, padding='same'))
  discriminator.add(BatchNormalization())
  discriminator.add(LeakyReLU(0.2))
  discriminator.add(Conv2D(512, kernel_size=5, strides=2, padding='same'))
  discriminator.add(BatchNormalization())
  discriminator.add(LeakyReLU(0.2))
  discriminator.add(Flatten())
  discriminator.add(Dense(1, activation='sigmoid'))
  return discriminator

"""## getting the data"""

(X_train, y_train), (X_test, y_test) = cifar10.load_data()

"""## Reshaping and normalizing the inputs"""

X_train = X_train.reshape(X_train.shape[0], 32, 32, 3)
X_test = X_test.reshape(X_test.shape[0], 32, 32, 3)
input_shape = (32, 32, 3)

# convert class vectors to binary class matrices
Y_train = np_utils.to_categorical(y_train, len(np.unique(y_train)))
Y_test = np_utils.to_categorical(y_test, len(np.unique(y_train)))

# the generator is using tanh activation, for which we need to preprocess 
# the image data into the range between -1 and 1.

X_train = np.float32(X_train)
X_train = (X_train / 255 - 0.5) * 2
X_train = np.clip(X_train, -1, 1)

X_test = np.float32(X_test)
X_test = (X_train / 255 - 0.5) * 2
X_test = np.clip(X_test, -1, 1)

"""## Generator model visualization"""

generator = generator_model()
discriminator = discriminator_model(X_train[0].shape)
generator.summary()
discriminator.compile(Adam(lr=0.0003, beta_1=0.5), loss='binary_crossentropy',metrics=['binary_accuracy'])

"""## Discriminator model visualization"""

discriminator.summary()

"""## compile model"""

discriminator.trainable = False
z = Input(shape=(100,))
generated_image = generator(z)
label = discriminator(generated_image)
GAN = Model(inputs=z, outputs=label)
GAN.compile(Adam(lr=0.0004, beta_1=0.5), loss='binary_crossentropy',metrics=['binary_accuracy'])

"""## fit model"""

epochs = 60
batch_size = 32
smooth = 0.1

real = np.ones(shape=(batch_size, 1))
fake = np.zeros(shape=(batch_size, 1))

d_loss = []
g_loss = []

for e in range(epochs + 1):
    for i in range(len(X_train) // batch_size):
        
        # Train Discriminator weights
        discriminator.trainable = True
        
        # Real samples
        X_batch = X_train[i*batch_size:(i+1)*batch_size]
        d_loss_real = discriminator.train_on_batch(x=X_batch, y=real * (1 - smooth))
        
        # Fake Samples
        z = np.random.normal(loc=0, scale=1, size=(batch_size, 100))
        X_fake = generator.predict_on_batch(z)
        d_loss_fake = discriminator.train_on_batch(x=X_fake, y=fake)
         
        # Discriminator loss
        d_loss_batch = 0.5 * (d_loss_real[0] + d_loss_fake[0])
        
        # Train Generator weights
        discriminator.trainable = False
        g_loss_batch = GAN.train_on_batch(x=z, y=real)

        print(
            'epoch = %d/%d, batch = %d/%d, d_loss=%.3f, g_loss=%.3f' % (e + 1, epochs, i, len(X_train) // batch_size, d_loss_batch, g_loss_batch[0]),
            100*' ',
            end='\r'
        )
    
    d_loss.append(d_loss_batch)
    g_loss.append(g_loss_batch[0])
    print('epoch = %d/%d, d_loss=%.3f, g_loss=%.3f' % (e + 1, epochs, d_loss[-1], g_loss[-1]), 100*' ')

    if e % 10 == 0:
        samples = 10
        x_fake = generator.predict(np.random.normal(loc=0, scale=1, size=(samples, 100)))

        for k in range(samples):
            plt.subplot(2, 5, k + 1, xticks=[], yticks=[])
            plt.imshow(((x_fake[k] + 1)* 127).astype(np.uint8))

        plt.tight_layout()
        plt.show()

"""## evaluate model"""

# plotting the metrics
plt.plot(d_loss)
plt.plot(g_loss)
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Discriminator', 'Adversarial'], loc='up right')
plt.show()

